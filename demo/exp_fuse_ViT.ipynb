{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a4ad41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50259007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\testings\\\\Python\\\\TestingPython\\\\ModelFusion'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: this cell can only be run once, since we're changing the directory.\n",
    "import sys\n",
    "import os\n",
    "\n",
    "path = \"D:/testings/Python/TestingPython/\"\n",
    "if path not in sys.path:\n",
    "    sys.path.append(path)\n",
    "\n",
    "# otfusion_path = \"D:/testings/Python/TestingPython/ModelFusion/otfusion/\"\n",
    "# if otfusion_path not in sys.path:\n",
    "#     sys.path.append(otfusion_path)\n",
    "os.chdir(\"../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18c520f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from ModelFusion.wasserstein_ensemble import (\n",
    "    geometric_ensembling_modularized,\n",
    "    get_wassersteinized_layers_modularized,\n",
    "    get_acts_wassersteinized_layers_modularized\n",
    ")\n",
    "from ModelFusion.helpers.utils import (\n",
    "    load_yml_file\n",
    ")\n",
    "from ModelFusion.helpers.load_model import reload_model\n",
    "from ModelFusion.helpers.model2graph import model2graph_wrapper, export_as_onnx\n",
    "from ModelFusion.wasserstein_ensemble_ViT import ViTFuser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26839c53",
   "metadata": {},
   "source": [
    "## Read args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a933101",
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_config_path = \"configs/fusion_configs.yml\"\n",
    "config = load_yml_file(fusion_config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0543ae3e",
   "metadata": {},
   "source": [
    "## Reload Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87101b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_1 = \"./clf_logs/2022_11_13_02_20_43_443389/\"\n",
    "model_path_2 = \"./clf_logs/2022_11_30_08_08_57_761624/\"\n",
    "model_name = \"ViT\"\n",
    "model_1 = reload_model(model_name, model_path_1)\n",
    "model_2 = reload_model(model_name, model_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "462d503c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViT(\n",
       "  (patch_embedding): PatchEmbeddingBlock(\n",
       "    (patch_embeddings): Conv2d(3, 768, kernel_size=(4, 4), stride=(4, 4), bias=False)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0): TransformerBlock(\n",
       "      (mlp): MLPBlock(\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (fn): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): SABlock(\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)\n",
       "        (out_rearrange): Rearrange('b h l d -> b l (h d)')\n",
       "        (drop_output): Dropout(p=0.0, inplace=False)\n",
       "        (drop_weights): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (mlp): MLPBlock(\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (fn): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): SABlock(\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)\n",
       "        (out_rearrange): Rearrange('b h l d -> b l (h d)')\n",
       "        (drop_output): Dropout(p=0.0, inplace=False)\n",
       "        (drop_weights): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (mlp): MLPBlock(\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (fn): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): SABlock(\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)\n",
       "        (out_rearrange): Rearrange('b h l d -> b l (h d)')\n",
       "        (drop_output): Dropout(p=0.0, inplace=False)\n",
       "        (drop_weights): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (mlp): MLPBlock(\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (fn): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): SABlock(\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)\n",
       "        (out_rearrange): Rearrange('b h l d -> b l (h d)')\n",
       "        (drop_output): Dropout(p=0.0, inplace=False)\n",
       "        (drop_weights): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (mlp): MLPBlock(\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (fn): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): SABlock(\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)\n",
       "        (out_rearrange): Rearrange('b h l d -> b l (h d)')\n",
       "        (drop_output): Dropout(p=0.0, inplace=False)\n",
       "        (drop_weights): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (mlp): MLPBlock(\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (fn): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): SABlock(\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)\n",
       "        (out_rearrange): Rearrange('b h l d -> b l (h d)')\n",
       "        (drop_output): Dropout(p=0.0, inplace=False)\n",
       "        (drop_weights): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (mlp): MLPBlock(\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (fn): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): SABlock(\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)\n",
       "        (out_rearrange): Rearrange('b h l d -> b l (h d)')\n",
       "        (drop_output): Dropout(p=0.0, inplace=False)\n",
       "        (drop_weights): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (mlp): MLPBlock(\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (fn): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): SABlock(\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)\n",
       "        (out_rearrange): Rearrange('b h l d -> b l (h d)')\n",
       "        (drop_output): Dropout(p=0.0, inplace=False)\n",
       "        (drop_weights): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (mlp): MLPBlock(\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (fn): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): SABlock(\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)\n",
       "        (out_rearrange): Rearrange('b h l d -> b l (h d)')\n",
       "        (drop_output): Dropout(p=0.0, inplace=False)\n",
       "        (drop_weights): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (mlp): MLPBlock(\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (fn): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): SABlock(\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)\n",
       "        (out_rearrange): Rearrange('b h l d -> b l (h d)')\n",
       "        (drop_output): Dropout(p=0.0, inplace=False)\n",
       "        (drop_weights): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (mlp): MLPBlock(\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (fn): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): SABlock(\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)\n",
       "        (out_rearrange): Rearrange('b h l d -> b l (h d)')\n",
       "        (drop_output): Dropout(p=0.0, inplace=False)\n",
       "        (drop_weights): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (mlp): MLPBlock(\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (fn): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): SABlock(\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)\n",
       "        (out_rearrange): Rearrange('b h l d -> b l (h d)')\n",
       "        (drop_output): Dropout(p=0.0, inplace=False)\n",
       "        (drop_weights): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (classification_head): Linear(in_features=768, out_features=10, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c962b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L = 768\n",
    "# ln = nn.LayerNorm(L)\n",
    "# list(ln.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a3c7e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token: torch.Size([1, 1, 768]), 3\n",
      "patch_embedding.position_embeddings: torch.Size([1, 64, 768]), 3\n",
      "patch_embedding.patch_embeddings.weight: torch.Size([768, 3, 4, 4]), 4\n",
      "blocks.0.mlp.linear1.weight: torch.Size([3072, 768]), 2\n",
      "blocks.0.mlp.linear2.weight: torch.Size([768, 3072]), 2\n",
      "blocks.0.norm1.weight: torch.Size([768]), 1\n",
      "blocks.0.norm1.bias: torch.Size([768]), 1\n",
      "blocks.0.attn.out_proj.weight: torch.Size([768, 768]), 2\n",
      "blocks.0.attn.qkv.weight: torch.Size([2304, 768]), 2\n",
      "blocks.0.norm2.weight: torch.Size([768]), 1\n",
      "blocks.0.norm2.bias: torch.Size([768]), 1\n",
      "blocks.1.mlp.linear1.weight: torch.Size([3072, 768]), 2\n",
      "blocks.1.mlp.linear2.weight: torch.Size([768, 3072]), 2\n",
      "blocks.1.norm1.weight: torch.Size([768]), 1\n",
      "blocks.1.norm1.bias: torch.Size([768]), 1\n",
      "blocks.1.attn.out_proj.weight: torch.Size([768, 768]), 2\n",
      "blocks.1.attn.qkv.weight: torch.Size([2304, 768]), 2\n",
      "blocks.1.norm2.weight: torch.Size([768]), 1\n",
      "blocks.1.norm2.bias: torch.Size([768]), 1\n",
      "blocks.2.mlp.linear1.weight: torch.Size([3072, 768]), 2\n",
      "blocks.2.mlp.linear2.weight: torch.Size([768, 3072]), 2\n",
      "blocks.2.norm1.weight: torch.Size([768]), 1\n",
      "blocks.2.norm1.bias: torch.Size([768]), 1\n",
      "blocks.2.attn.out_proj.weight: torch.Size([768, 768]), 2\n",
      "blocks.2.attn.qkv.weight: torch.Size([2304, 768]), 2\n",
      "blocks.2.norm2.weight: torch.Size([768]), 1\n",
      "blocks.2.norm2.bias: torch.Size([768]), 1\n",
      "blocks.3.mlp.linear1.weight: torch.Size([3072, 768]), 2\n",
      "blocks.3.mlp.linear2.weight: torch.Size([768, 3072]), 2\n",
      "blocks.3.norm1.weight: torch.Size([768]), 1\n",
      "blocks.3.norm1.bias: torch.Size([768]), 1\n",
      "blocks.3.attn.out_proj.weight: torch.Size([768, 768]), 2\n",
      "blocks.3.attn.qkv.weight: torch.Size([2304, 768]), 2\n",
      "blocks.3.norm2.weight: torch.Size([768]), 1\n",
      "blocks.3.norm2.bias: torch.Size([768]), 1\n",
      "blocks.4.mlp.linear1.weight: torch.Size([3072, 768]), 2\n",
      "blocks.4.mlp.linear2.weight: torch.Size([768, 3072]), 2\n",
      "blocks.4.norm1.weight: torch.Size([768]), 1\n",
      "blocks.4.norm1.bias: torch.Size([768]), 1\n",
      "blocks.4.attn.out_proj.weight: torch.Size([768, 768]), 2\n",
      "blocks.4.attn.qkv.weight: torch.Size([2304, 768]), 2\n",
      "blocks.4.norm2.weight: torch.Size([768]), 1\n",
      "blocks.4.norm2.bias: torch.Size([768]), 1\n",
      "blocks.5.mlp.linear1.weight: torch.Size([3072, 768]), 2\n",
      "blocks.5.mlp.linear2.weight: torch.Size([768, 3072]), 2\n",
      "blocks.5.norm1.weight: torch.Size([768]), 1\n",
      "blocks.5.norm1.bias: torch.Size([768]), 1\n",
      "blocks.5.attn.out_proj.weight: torch.Size([768, 768]), 2\n",
      "blocks.5.attn.qkv.weight: torch.Size([2304, 768]), 2\n",
      "blocks.5.norm2.weight: torch.Size([768]), 1\n",
      "blocks.5.norm2.bias: torch.Size([768]), 1\n",
      "blocks.6.mlp.linear1.weight: torch.Size([3072, 768]), 2\n",
      "blocks.6.mlp.linear2.weight: torch.Size([768, 3072]), 2\n",
      "blocks.6.norm1.weight: torch.Size([768]), 1\n",
      "blocks.6.norm1.bias: torch.Size([768]), 1\n",
      "blocks.6.attn.out_proj.weight: torch.Size([768, 768]), 2\n",
      "blocks.6.attn.qkv.weight: torch.Size([2304, 768]), 2\n",
      "blocks.6.norm2.weight: torch.Size([768]), 1\n",
      "blocks.6.norm2.bias: torch.Size([768]), 1\n",
      "blocks.7.mlp.linear1.weight: torch.Size([3072, 768]), 2\n",
      "blocks.7.mlp.linear2.weight: torch.Size([768, 3072]), 2\n",
      "blocks.7.norm1.weight: torch.Size([768]), 1\n",
      "blocks.7.norm1.bias: torch.Size([768]), 1\n",
      "blocks.7.attn.out_proj.weight: torch.Size([768, 768]), 2\n",
      "blocks.7.attn.qkv.weight: torch.Size([2304, 768]), 2\n",
      "blocks.7.norm2.weight: torch.Size([768]), 1\n",
      "blocks.7.norm2.bias: torch.Size([768]), 1\n",
      "blocks.8.mlp.linear1.weight: torch.Size([3072, 768]), 2\n",
      "blocks.8.mlp.linear2.weight: torch.Size([768, 3072]), 2\n",
      "blocks.8.norm1.weight: torch.Size([768]), 1\n",
      "blocks.8.norm1.bias: torch.Size([768]), 1\n",
      "blocks.8.attn.out_proj.weight: torch.Size([768, 768]), 2\n",
      "blocks.8.attn.qkv.weight: torch.Size([2304, 768]), 2\n",
      "blocks.8.norm2.weight: torch.Size([768]), 1\n",
      "blocks.8.norm2.bias: torch.Size([768]), 1\n",
      "blocks.9.mlp.linear1.weight: torch.Size([3072, 768]), 2\n",
      "blocks.9.mlp.linear2.weight: torch.Size([768, 3072]), 2\n",
      "blocks.9.norm1.weight: torch.Size([768]), 1\n",
      "blocks.9.norm1.bias: torch.Size([768]), 1\n",
      "blocks.9.attn.out_proj.weight: torch.Size([768, 768]), 2\n",
      "blocks.9.attn.qkv.weight: torch.Size([2304, 768]), 2\n",
      "blocks.9.norm2.weight: torch.Size([768]), 1\n",
      "blocks.9.norm2.bias: torch.Size([768]), 1\n",
      "blocks.10.mlp.linear1.weight: torch.Size([3072, 768]), 2\n",
      "blocks.10.mlp.linear2.weight: torch.Size([768, 3072]), 2\n",
      "blocks.10.norm1.weight: torch.Size([768]), 1\n",
      "blocks.10.norm1.bias: torch.Size([768]), 1\n",
      "blocks.10.attn.out_proj.weight: torch.Size([768, 768]), 2\n",
      "blocks.10.attn.qkv.weight: torch.Size([2304, 768]), 2\n",
      "blocks.10.norm2.weight: torch.Size([768]), 1\n",
      "blocks.10.norm2.bias: torch.Size([768]), 1\n",
      "blocks.11.mlp.linear1.weight: torch.Size([3072, 768]), 2\n",
      "blocks.11.mlp.linear2.weight: torch.Size([768, 3072]), 2\n",
      "blocks.11.norm1.weight: torch.Size([768]), 1\n",
      "blocks.11.norm1.bias: torch.Size([768]), 1\n",
      "blocks.11.attn.out_proj.weight: torch.Size([768, 768]), 2\n",
      "blocks.11.attn.qkv.weight: torch.Size([2304, 768]), 2\n",
      "blocks.11.norm2.weight: torch.Size([768]), 1\n",
      "blocks.11.norm2.bias: torch.Size([768]), 1\n",
      "norm.weight: torch.Size([768]), 1\n",
      "norm.bias: torch.Size([768]), 1\n",
      "classification_head.weight: torch.Size([10, 768]), 2\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_1.named_parameters():\n",
    "    print(f\"{name}: {param.shape}, {param.ndim}\")\n",
    "#     if \"norm\" in name:\n",
    "#         print(param[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0b8797",
   "metadata": {},
   "source": [
    "## Fuse non-fc layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68f5e803",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuser = ViTFuser([model_1, model_2], config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d2ff8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "returns a uniform measure of cardinality:  768\n",
      "returns a uniform measure of cardinality:  768\n",
      "the transport map is  tensor([[3.3085e-07, 3.8550e-11, 1.1301e-17,  ..., 3.8042e-11, 2.7480e-11,\n",
      "         2.4980e-11],\n",
      "        [3.6191e-11, 1.1701e-12, 3.8227e-09,  ..., 1.0288e-11, 1.1302e-08,\n",
      "         5.9800e-14],\n",
      "        [3.8945e-14, 2.8679e-16, 4.5270e-12,  ..., 1.9598e-15, 1.9551e-14,\n",
      "         6.6230e-13],\n",
      "        ...,\n",
      "        [4.2885e-11, 3.5610e-12, 1.3808e-13,  ..., 4.0169e-10, 7.8354e-11,\n",
      "         4.3993e-08],\n",
      "        [2.0361e-09, 4.8177e-04, 5.7465e-15,  ..., 9.1433e-10, 9.5169e-08,\n",
      "         6.2155e-12],\n",
      "        [1.1476e-12, 1.3175e-16, 1.2533e-14,  ..., 3.0473e-11, 4.7878e-05,\n",
      "         1.7008e-10]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0006, device='cuda:0')\n",
      "Here, trace is 0.0005825987318530679 and matrix sum is 1.0 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "patch_embedding.patch_embeddings.weight: cuda:0, torch.Size([768, 3, 4, 4])\n",
      "cls_token: cuda:0, torch.Size([1, 1, 768])\n",
      "patch_embedding.position_embeddings: cuda:0, torch.Size([1, 64, 768])\n"
     ]
    }
   ],
   "source": [
    "aligned_weights_dict = fuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301f37e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IML",
   "language": "python",
   "name": "iml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
